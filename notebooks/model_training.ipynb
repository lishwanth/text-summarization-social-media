{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Model Training\n", "This notebook is used for training the T5 model for text summarization."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n", "from datasets import load_dataset\n", "\n", "# Load the dataset\n", "dataset = load_dataset('cnn_dailymail', '3.0.0', split='train')\n", "\n", "# Initialize the tokenizer and model\n", "model_name = \"t5-small\"\n", "tokenizer = T5Tokenizer.from_pretrained(model_name)\n", "model = T5ForConditionalGeneration.from_pretrained(model_name)\n", "\n", "# Tokenize the dataset\n", "def preprocess_function(examples):\n", "    inputs = [f\"summarize: {text}\" for text in examples['article']]\n", "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n", "\n", "    # Tokenize labels\n", "    with tokenizer.as_target_tokenizer():\n", "        labels = tokenizer(examples['highlights'], max_length=150, truncation=True)\n", "\n", "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n", "    return model_inputs\n", "\n", "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n", "\n", "# Set training arguments\n", "training_args = TrainingArguments(\n", "    output_dir=\"./models/summarization_model\",\n", "    evaluation_strategy=\"epoch\",\n", "    learning_rate=2e-5,\n", "    per_device_train_batch_size=8,\n", "    num_train_epochs=3,\n", "    weight_decay=0.01,\n", "    save_total_limit=2,\n", ")\n", "\n", "# Initialize the Trainer\n", "trainer = Trainer(\n", "    model=model,\n", "    args=training_args,\n", "    train_dataset=tokenized_dataset,\n", ")\n", "\n", "# Train the model\n", "trainer.train()\n", "\n", "# Save the model\n", "trainer.save_model(\"../models/summarization_model\")"]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 4}